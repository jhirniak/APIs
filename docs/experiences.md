# Freebies:
* Street view
* Geocoding in Google Maps, Google Places, 4Square
* 360 videos
* Music (e.g. spotify, rdio) + lyrics (karaoke)
* Wiki
* WolframAlpha
* IBM Watson
* Historia (streszczenie/os czasu)
* Flicker (wiecej zdjec)
* Weather
* Unit conversion
* Place -> Famous people -> Quotes
* News

## Movies
* Get move info from IMDB ->
* play trailer from Netflix - option to watch whole ->
* trip to the place of recordings ->
* facts about actors ->
* interviews and extra content, etc.
* News

## Academia
* Related areas
* Main topics
* Related sources
* News related to topic

## Places
* News from that area


# Ads:
* Trailers
* Loty
* Hotele
* Miejscowe rzeczy (restauracje, jedzenie, muzyka, etc.)
* Uber, cityplaner, hailo
* Currency shops and rates
* House sales (+ crime data?)

# How we experience?
So, I've been reading [this article on how fb describes what's in photos to blind people](http://techcrunch.com/2015/10/13/facebooks-working-on-a-tool-to-help-the-blind-see-images/#.wqgozt:BuEF), and thought how we all differently are used to experience certain things (or in same cases what are limitations are). If we are going to have summary module and object recognition API then why not add blind mode which will also read what's in the picture (for instance). Or based on the experience processing could say more about not just what is in the photo, but in the picture there is Edinburgh Castle (built in... blabla to which X was on 12/05/2015), etc.

  * [article](http://techcrunch.com/2015/10/13/facebooks-working-on-a-tool-to-help-the-blind-see-images/#.wqgozt:BuEF)
